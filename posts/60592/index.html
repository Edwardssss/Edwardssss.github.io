<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>torch.nn入门 | 砕月之殇的摆烂小窝</title><meta name="author" content="砕月之殇"><meta name="copyright" content="砕月之殇"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PyTorch 提供设计优雅的模块和类torch.nn，torch.optim，Dataset 和 DataLoader来帮助您创建和训练神经网络 为了充分利用它们的功能并针对您的问题对其进行自定义，您需要真正地了解他们的工作 为了建立这种理解，我们将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能； 我们最初只会使用最基本的 PyTorch 张量功能。 然后，我们将一次">
<meta property="og:type" content="article">
<meta property="og:title" content="torch.nn入门">
<meta property="og:url" content="https://edwardssss.github.io/posts/60592/index.html">
<meta property="og:site_name" content="砕月之殇的摆烂小窝">
<meta property="og:description" content="PyTorch 提供设计优雅的模块和类torch.nn，torch.optim，Dataset 和 DataLoader来帮助您创建和训练神经网络 为了充分利用它们的功能并针对您的问题对其进行自定义，您需要真正地了解他们的工作 为了建立这种理解，我们将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能； 我们最初只会使用最基本的 PyTorch 张量功能。 然后，我们将一次">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://edwardssss.github.io/user/show_myself.png">
<meta property="article:published_time" content="2023-03-07T01:02:12.000Z">
<meta property="article:modified_time" content="2023-03-07T01:39:24.949Z">
<meta property="article:author" content="砕月之殇">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://edwardssss.github.io/user/show_myself.png"><link rel="shortcut icon" href="/user/show_myself.png"><link rel="canonical" href="https://edwardssss.github.io/posts/60592/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="codeva-CoYzr5u1V7"/><meta name="google-site-verification" content="KPZaFjnh6c0QSJx-S-QSmEDT2OkVNiAxlmT5JDJfj_A"/><meta name="msvalidate.01" content="732F147F330E6611EE583F63DC9C01C3"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20460ef387a66bb64cb9ae71e37ec747";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'torch.nn入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-07 09:39:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/c/font_3870982_w1axi7vkykm.css"><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="砕月之殇的摆烂小窝" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/user/show_myself.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-yu"></i><span> 摸鱼</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 页游</span></a></li><li><a class="site-page child" href="/ghs/"><i class="fa-fw fa fa-user-lock"></i><span> GHS</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-ren111"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/user/top_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="砕月之殇的摆烂小窝"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-yu"></i><span> 摸鱼</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fas fa-gamepad"></i><span> 页游</span></a></li><li><a class="site-page child" href="/ghs/"><i class="fa-fw fa fa-user-lock"></i><span> GHS</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-ren111"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">torch.nn入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-07T01:02:12.000Z" title="发表于 2023-03-07 09:02:12">2023-03-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-07T01:39:24.949Z" title="更新于 2023-03-07 09:39:24">2023-03-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="torch.nn入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/60592/#post-comment"><span class="waline-comment-count" data-path="/posts/60592/"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>PyTorch 提供设计优雅的模块和类<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a>，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a>，<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset">Dataset</a> 和 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">DataLoader</a>来帮助您创建和训练神经网络</p>
<p>为了充分利用它们的功能并针对您的问题对其进行自定义，您需要真正地了解他们的工作</p>
<p>为了建立这种理解，我们将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能； 我们最初只会使用最基本的 PyTorch 张量功能。</p>
<p>然后，我们将一次从<code>torch.nn</code>，<code>torch.optim</code>，<code>Dataset</code>或<code>DataLoader</code>中逐个添加一个功能，确切地显示每个功能，以及如何使代码更简洁或更灵活</p>
<p>本教程假定您已经安装了 PyTorch，并且熟悉张量操作的基础知识</p>
<p>(如果您熟悉 Numpy 数组操作，将会发现此处使用的 PyTorch 张量操作几乎相同）</p>
<h2 id="MNIST-数据设置"><a class="header-anchor" href="#MNIST-数据设置"> </a>MNIST 数据设置</h2>
<hr>
<p>我们将使用经典的<a target="_blank" rel="noopener" href="http://deeplearning.net/data/mnist/">MNIST</a>数据集，该数据集由手绘数字的黑白图像组成(介于 0 到 9 之间）</p>
<p>我们将使用<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/pathlib.html">pathlib</a> 处理路径(Python 3 标准库的一部分），并下载数据集</p>
<p>我们只会在使用模块时才导入它们，因此您可以确切地看到正在使用模块的每个细节</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">&quot;mnist&quot;</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">&quot;http://deeplearning.net/data/mnist/&quot;</span></span><br><span class="line">FILENAME = <span class="string">&quot;mnist.pkl.gz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).<span class="built_in">open</span>(<span class="string">&quot;wb&quot;</span>).write(content)</span><br></pre></td></tr></table></figure>
<p>该数据集为 numpy 数组格式，并已使用 pickle(一种用于序列化数据的 python 特定格式）存储</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>((PATH / FILENAME).as_posix(), <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>每个图像为 28 x 28，并存储被拍平长度为 784(= 28x28）的向量，让我们来看一个</p>
<p>我们需要先将其重塑为 2d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&quot;gray&quot;</span>) <span class="comment"># 选取第一个数据查看</span></span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br></pre></td></tr></table></figure>
<p>正常输出应该是一个数组和MNIST包当中的随机的一张图片，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">50000</span>, <span class="number">784</span>)</span><br></pre></td></tr></table></figure>
<img src="/posts/60592/exp.png" class="">
<p>我们可以看到输出了(50000,784)，这表明包内有50000个数据</p>
<p>PyTorch 使用<code>torch.tensor</code>而不是 numpy 数组，因此我们需要转换数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = <span class="built_in">map</span>(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.<span class="built_in">min</span>(), y_train.<span class="built_in">max</span>()</span><br><span class="line"><span class="built_in">print</span>(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.<span class="built_in">min</span>(), y_train.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>
<p>正常的输出应该是两个张量内容及张量的尺寸和最大维度，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]) tensor([<span class="number">5</span>, <span class="number">0</span>, <span class="number">4</span>,  ..., <span class="number">8</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">torch.Size([<span class="number">50000</span>, <span class="number">784</span>])</span><br><span class="line">tensor(<span class="number">0</span>) tensor(<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="从零开始的神经网络-无-torch-nn）"><a class="header-anchor" href="#从零开始的神经网络-无-torch-nn）"> </a>从零开始的神经网络(无 torch.nn）</h2>
<hr>
<p>首先，我们仅使用 PyTorch 张量操作创建模型。 我们假设您已经熟悉神经网络的基础知识。 (如果您不是，则可以在<a target="_blank" rel="noopener" href="https://course.fast.ai/">course.fast.ai</a>中学习它们）。</p>
<p>PyTorch 提供了创建随机或零填充张量的方法，我们将使用它们来为简单的线性模型创建权重和偏差。 这些只是常规张量，还有一个非常特殊的附加值：我们告诉 PyTorch 它们需要梯度。 这使 PyTorch 记录了在张量上完成的所有操作，因此它可以在反向传播时自动地计算梯度！</p>
<p>对于权重，我们在初始化之后设置<code>requires_grad</code>，因为我们不希望该步骤包含在梯度中。 (请注意，PyTorch 中的尾随_表示该操作是就地执行的。）</p>
<p>我们在这里用<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier</a>初始化(通过乘以 1 / sqrt(n））来初始化权重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>由于 PyTorch 具有自动计算梯度的功能，我们可以将任何标准的 Python 函数(或可调用对象）用作模型</p>
<p>因此，让我们编写一个简单的矩阵乘法和广播加法来创建一个简单的线性模型</p>
<p>我们还需要激活函数，因此我们将编写并使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">log\_softmax</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0044em;vertical-align:-0.31em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span></span></span></span></p>
<p>请记住：尽管 PyTorch 提供了许多预先编写的损失函数，激活函数等，但是您可以使用纯 Python 轻松编写自己的函数</p>
<p>PyTorch 甚至会自动为您的函数创建快速 GPU 或矢量化的 CPU 代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x - x.exp().<span class="built_in">sum</span>(-<span class="number">1</span>).log().unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>
<p>在上面，<code>@</code>代表点积运算。 我们将对一批数据(在这种情况下为 64 张图像）调用函数。 这是一个前向传播</p>
<p>请注意，由于我们从随机权重开始，因此在这一阶段，我们的预测不会比随机预测更好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line"><span class="built_in">print</span>(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">2.5454</span>, -<span class="number">2.5716</span>, -<span class="number">1.7979</span>, -<span class="number">2.6673</span>, -<span class="number">2.4757</span>, -<span class="number">2.4538</span>, -<span class="number">2.1775</span>, -<span class="number">1.9078</span>,</span><br><span class="line">        -<span class="number">2.1895</span>, -<span class="number">2.7136</span>], grad_fn=&lt;SelectBackward0&gt;) torch.Size([<span class="number">64</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>如您所见，<code>preds</code>张量不仅包含张量值，还包含梯度函数。 稍后我们将使用它进行反向传播</p>
<p>让我们实现负对数似然作为损失函数(同样，我们只能使用标准 Python）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nll</span>(<span class="params"><span class="built_in">input</span>, target</span>):</span><br><span class="line">    <span class="keyword">return</span> -<span class="built_in">input</span>[<span class="built_in">range</span>(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>
<p>让我们用随机模型来检查损失，以便我们以后看向后传播后是否可以改善</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line"><span class="built_in">print</span>(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.3733</span>, grad_fn=&lt;NegBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们还实现一个函数来计算模型的准确性。 对于每个预测，如果具有最大值的索引与目标值匹配，则该预测是正确的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">out, yb</span>):</span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>让我们检查一下随机模型的准确性，以便我们可以看出随着损失的增加，准确性是否有所提高</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0469</span>)</span><br></pre></td></tr></table></figure>
<p>现在，我们可以运行一个训练循环</p>
<p>对于每次迭代，我们将：</p>
<ul>
<li>选择一个小批量数据(大小为<code>bs</code>）</li>
<li>使用模型进行预测</li>
<li>计算损失</li>
<li><code>loss.backward()</code>更新模型的梯度，在这种情况下为<code>weights</code>和<code>bias</code></li>
</ul>
<p>现在，我们使用这些梯度来更新权重和偏差</p>
<p>我们在<code>torch.no_grad()</code>上下文管理器中执行此操作，因为我们不希望在下一步的梯度计算中记录这些操作</p>
<p>您可以在上阅读有关 PyTorch 的 Autograd 如何记录操作的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/autograd.html">更多信息</a></p>
<p>然后，将梯度设置为零，以便为下一个循环做好准备</p>
<p>否则，我们的梯度会记录所有已发生操作的运行记录(即<code>loss.backward()</code>将梯度添加到已存储的内容中，而不是替换它们）</p>
<p>可以使用标准的 python 调试器逐步浏览 PyTorch 代码，从而可以在每一步检查各种变量值</p>
<p>取消注释以下<code>set_trace()</code>即可尝试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>就是这样：我们完全从头开始创建并训练了一个最小的神经网络(在这种情况下，是逻辑回归，因为我们没有隐藏的层）</p>
<p>让我们检查损失和准确性，并将其与我们之前获得的进行比较</p>
<p>我们希望损失会减少，准确性会增加，而且确实如此</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0803</span>, grad_fn=&lt;NegBackward0&gt;) tensor(<span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用-torch-nn-functional"><a class="header-anchor" href="#使用-torch-nn-functional"> </a>使用 torch.nn.functional</h2>
<hr>
<p>现在，我们将重构代码，使其与以前相同，只是我们将开始利用 PyTorch 的<code>nn</code>类使其更加简洁和灵活</p>
<p>从这里开始的每一步，我们都应该使代码达成一个或多个：更短，更易理解和/或更灵活</p>
<p>第一步也是最简单的步骤，就是用<code>torch.nn.functional</code>(通常按照惯例将其导入到名称空间F中）替换我们的手写激活和损失函数，从而缩短代码长度</p>
<p>该模块包含<code>torch.nn</code>库中的所有函数(而该库的其他部分包含类）</p>
<p>除了广泛的损失和激活函数外，您还会在这里找到一些合适的函数来创建神经网络，例如池化函数</p>
<p>还有一些用于进行卷积，线性图层等的函数，但是正如我们将看到的那样，通常可以使用库的其他部分来更好地处理这些函数</p>
<p>如果您使用的是负对数似然损失和 log softmax 激活，那么 Pytorch 会提供将两者结合的单个函数<code>F.cross_entropy</code></p>
<p>因此，我们甚至可以从模型中删除激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>
<p>请注意，我们不再在model函数中调用log_softmax</p>
<p>让我们确认我们的损失和准确性与以前相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0803</span>, grad_fn=&lt;NllLossBackward0&gt;) tensor(<span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用-nn-Module-进行重构"><a class="header-anchor" href="#使用-nn-Module-进行重构"> </a>使用 nn.Module 进行重构</h2>
<hr>
<p>接下来，我们将使用<code>nn.Module</code>和<code>nn.Parameter</code>进行更清晰，更简洁的训练循环</p>
<p>我们将<code>nn.Module</code>子类化(它本身是一个类并且能够跟踪状态）</p>
<p>在这种情况下，我们要创建一个类，该类包含前进步骤的权重，偏差和方法</p>
<p><code>nn.Module</code>具有许多我们将要使用的属性和方法，例如<code>.parameters()</code>和<code>.zero_grad()</code></p>
<p><code>nn.Module</code>(大写 M）是 PyTorch 的特定概念，也是我们将经常使用的一个类</p>
<p><code>nn.Module</code>不要与(小写m）模块的 Python 概念混淆，该模块是可以导入的 Python 代码文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>
<p>由于我们现在使用的是对象而不是仅使用函数，因此我们首先必须实例化模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>
<p>现在我们可以像以前一样计算损失</p>
<p>请注意，<code>nn.Module</code>对象的使用就像它们是函数一样(即，它们是可调用的）</p>
<p>但是在后台 Pytorch 会自动调用我们的<code>forward</code>方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.4099</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>以前，在我们的训练循环中，我们必须按名称更新每个参数的值</p>
<p>并手动将每个参数的 grads 分别归零，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    weights -= weights.grad * lr</span><br><span class="line">    bias -= bias.grad * lr</span><br><span class="line">    weights.grad.zero_()</span><br><span class="line">    bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>现在我们可以利用 model.parameters(）和 model.zero_grad(）</p>
<p>它们都由 PyTorch 为<code>nn.Module</code>定义</p>
<p>从而使这些步骤更简洁，并且更不会出现忘记某些参数的错误</p>
<p>特别是当我们有一个更复杂的模型的时候</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters(): </span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">type</span>(p),p,p.grad)</span><br><span class="line">        p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>我们将把小的训练循环包装在<code>fit</code>函数中，以便稍后再运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line">                </span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>
<p>让我们仔细检查一下我们的损失是否下降了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0806</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="使用-nn-Linear-重构"><a class="header-anchor" href="#使用-nn-Linear-重构"> </a>使用 nn.Linear 重构</h2>
<hr>
<p>我们继续重构我们的代码</p>
<p>代替手动定义和初始化<code>self.weights</code>和<code>self.bias</code>并计算<code>xb @ self.weights + self.bias</code>，我们将对线性层使用 Pytorch 类 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers">nn.Linear</a> ，这将为我们完成所有工作</p>
<p>Pytorch 具有许多类型的预定义层，可以大大简化我们的代码，并且通常也可以使其速度更快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>
<p>我们用与以前相同的方式实例化模型并计算损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.2902</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们仍然可以使用与以前相同的fit方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fit()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0812</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="使用优化重构"><a class="header-anchor" href="#使用优化重构"> </a>使用优化重构</h2>
<hr>
<p>Pytorch 还提供了一个包含各种优化算法的软件包<code>torch.optim</code></p>
<p>我们可以使用优化器中的<code>step</code>方法采取向前的步骤，而不是手动更新每个参数</p>
<p>这就是我们将要替换之前手动编码的优化步骤</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters(): p -= p.grad * lr</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<p>我们只需使用下面的代替</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">opt.step()</span><br><span class="line">opt.zero_grad()</span><br></pre></td></tr></table></figure>
<p><code>optim.zero_grad()</code>将梯度重置为 0，我们需要在计算下一个小批量的梯度之前调用它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>
<p>我们将定义一个小函数来创建模型和优化器，以便将来再次使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>():</span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">model, opt = get_model()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.3324</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">0.0821</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="使用数据集进行重构"><a class="header-anchor" href="#使用数据集进行重构"> </a>使用数据集进行重构</h2>
<hr>
<p>PyTorch 有一个抽象的 Dataset 类</p>
<p>数据集可以是具有<code>__len__</code>函数(由 Python 的标准<code>len</code>函数调用）和具有<code>__getitem__</code>函数作为对其进行索引的一种方法</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">本教程</a>演示了一个不错的示例，该示例创建一个自定义<code>FacialLandmarkDataset</code>类作为<code>Dataset</code>的子类</p>
<p>PyTorch 的<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset">TensorDataset</a>是一个数据集包装张量。 通过定义索引的长度和方式，这也为我们提供了沿张量的一维进行迭代，索引和切片的方法</p>
<p>这将使我们在训练的同一行中更容易访问自变量和因变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>
<p><code>x_train</code>和<code>y_train</code>都可以合并为一个<code>TensorDataset</code>，这将更易于迭代和切片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>以前，我们不得不分别遍历 x 和 y 值的迷你批处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xb = x_train[start_i:end_i]</span><br><span class="line">yb = y_train[start_i:end_i]</span><br></pre></td></tr></table></figure>
<p>现在，我们可以将两个步骤一起执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xb,yb = train_ds[i * bs : i * bs + bs]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0833</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="使用-DataLoader-进行重构"><a class="header-anchor" href="#使用-DataLoader-进行重构"> </a>使用 DataLoader 进行重构</h2>
<hr>
<p>Pytorch的<code>DataLoader</code>负责批次管理。 您可以从任何<code>Dataset</code>创建一个<code>DataLoader</code></p>
<p><code>DataLoader</code>使迭代迭代变得更加容易</p>
<p>不必使用<code>train_ds[i * bs : i * bs + bs]</code>，<code>DataLoader</code>会自动为我们提供每个小批量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>
<p>以前，我们的循环遍历批处理(xb，yb），如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n-<span class="number">1</span>)//bs + <span class="number">1</span>):</span><br><span class="line">    xb,yb = train_ds[i*bs : i*bs+bs]</span><br><span class="line">    pred = model(xb)</span><br></pre></td></tr></table></figure>
<p>现在，我们的循环更加简洁了，因为(xb，yb）是从数据加载器自动加载的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> xb,yb <span class="keyword">in</span> train_dl:</span><br><span class="line">    pred = model(xb)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.0820</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>得益于 Pytorch的<code>nn.Module</code>，<code>nn.Parameter</code>，<code>Dataset</code>和<code>DataLoader</code>，我们的训练循环现在变得更小，更容易理解</p>
<p>现在，让我们尝试添加在实践中创建有效模型所需的基本功能</p>
<h2 id="添加验证"><a class="header-anchor" href="#添加验证"> </a>添加验证</h2>
<hr>
<p>在第 1 部分中，我们只是试图建立一个合理的训练循环以用于我们的训练数据</p>
<p>实际上，您总是也应该具有<a target="_blank" rel="noopener" href="https://www.fast.ai/2017/11/13/validation-sets/">验证集</a>，以便识别您是否过度拟合</p>
<p>打乱训练数据顺序对于防止批次与过度拟合之间的相关性很重要</p>
<p>另一方面，无论我们是否打乱验证集，验证损失都是相同的</p>
<p>由于打乱顺序需要花费更多时间，因此打乱验证集数据顺序没有任何意义</p>
<p>我们将验证集的批次大小设为训练集的两倍</p>
<p>这是因为验证集不需要反向传播，因此占用的内存更少(不需要存储渐变）</p>
<p>我们利用这一优势来使用更大的批量，并更快地计算损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>我们将在每个 epoch 结束时计算并打印验证损失</p>
<p>(请注意，我们总是在训练之前调用<code>model.train()</code>，并在推断之前调用<code>model.eval()</code>，因为诸如<code>nn.BatchNorm2d</code>和<code>nn.Dropout</code>之类的图层会使用它们，以确保这些不同阶段的行为正确）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line">    <span class="built_in">print</span>(epoch, valid_loss / <span class="built_in">len</span>(valid_dl))</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> tensor(<span class="number">0.3302</span>)</span><br><span class="line"><span class="number">1</span> tensor(<span class="number">0.2727</span>)</span><br></pre></td></tr></table></figure>
<h2 id="创建-fit-）和-get-data-）"><a class="header-anchor" href="#创建-fit-）和-get-data-）"> </a>创建 fit(）和 get_data(）</h2>
<hr>
<p>现在，我们将自己进行一些重构</p>
<p>由于我们经历了两次相似的过程来计算训练集和验证集的损失，因此我们将其设为自己的函数loss_batch，该函数可计算一批损失</p>
<p>我们将优化器传入训练集中，并使用它执行反向传播</p>
<p>对于验证集，我们没有通过优化程序，因此该方法不会执行反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model, loss_func, xb, yb, opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss.item(), <span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p><code>fit</code>运行必要的操作来训练我们的模型，并计算每个时期的训练和验证损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs, model, loss_func, opt, train_dl, valid_dl</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = <span class="built_in">zip</span>(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses, nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="built_in">print</span>(epoch, val_loss)</span><br></pre></td></tr></table></figure>
<p><code>get_data</code>返回用于训练和验证集的数据加载器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">train_ds, valid_ds, bs</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>现在，我们获取数据加载器和拟合模型的整个过程可以在3行代码中运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">0.319864363270998</span></span><br><span class="line"><span class="number">1</span> <span class="number">0.2773645614147186</span></span><br></pre></td></tr></table></figure>
<p>您可以使用这些基本的 3 行代码来训练各种各样的模型</p>
<p>让我们看看是否可以使用它们来训练卷积神经网络(CNN）</p>
<h2 id="切换到-CNN"><a class="header-anchor" href="#切换到-CNN"> </a>切换到 CNN</h2>
<hr>
<p>现在，我们将构建具有三个卷积层的神经网络</p>
<p>由于上一节中的所有函数都不包含任何有关模型组合的内容，因此我们将能够使用它们来训练 CNN，而无需进行任何修改</p>
<p>我们将使用 Pytorch 的预定义<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d">Conv2d</a>类作为我们的卷积层</p>
<p>我们定义具有 3 个卷积层的 CNN。 每个卷积后跟一个 ReLU</p>
<p>最后，我们执行平均池化(请注意，<code>view</code>是 numpy 的<code>reshape</code>的 PyTorch 版本）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        xb = xb.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(-<span class="number">1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-3/#sgd">动量</a>是随机梯度下降的一种变体，它也考虑了以前的更新，通常可以加快训练速度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">0.40830671231746674</span></span><br><span class="line"><span class="number">1</span> <span class="number">0.2581173546910286</span></span><br></pre></td></tr></table></figure>
<h2 id="nn-Sequential"><a class="header-anchor" href="#nn-Sequential"> </a>nn.Sequential</h2>
<hr>
<p>torch.nn还有另一个灵活的类，可以用来简化我们的代码：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential">Sequential</a></p>
<p><code>Sequential</code>对象以顺序方式运行其中包含的每个模块。 这是编写神经网络的一种简单方法</p>
<p>要利用此优势，我们需要能够从给定的函数轻松定义自定义层</p>
<p>例如，PyTorch 没有视图图层，我们需要为网络创建一个图层</p>
<p><code>Lambda</code>将创建一个层，然后在使用<code>Sequential</code>定义网络时可以使用该层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Lambda</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>用<code>Sequential</code>创建的模型很简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">0.39486207270622253</span></span><br><span class="line"><span class="number">1</span> <span class="number">0.21907853271365166</span></span><br></pre></td></tr></table></figure>
<h2 id="包装-DataLoader"><a class="header-anchor" href="#包装-DataLoader"> </a>包装 DataLoader</h2>
<hr>
<p>虽然我们的 CNN 网络很简洁，但是它只能在 MNIST 数据集上面有效，因为</p>
<ul>
<li>MNIST 数据集假设输入为 28 * 28 长向量</li>
<li>MNIST 数据集假设 CNN 的最终网格尺寸为 4 * 4(这是因为我们使用的平均池化卷积核的大小）</li>
</ul>
<p>让我们摆脱这两个假设，因此我们的模型需要适用于任何 2d 单通道图像</p>
<p>首先，我们可以删除初始的 Lambda 层，并将数据预处理移至生成器中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrappedDataLoader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, func</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        batches = <span class="built_in">iter</span>(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>接下来，我们可以将<code>nn.AvgPool2d</code>替换为<code>nn.AdaptiveAvgPool2d</code>，这使我们可以定义所需的输出张量的大小，而不是所需的输入张量的大小</p>
<p>结果，我们的模型将适用于任何大小的输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>试试看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">0.3495198380470276</span></span><br><span class="line"><span class="number">1</span> <span class="number">0.2577100547194481</span></span><br></pre></td></tr></table></figure>
<h2 id="使用您的-GPU"><a class="header-anchor" href="#使用您的-GPU"> </a>使用您的 GPU</h2>
<hr>
<p>如果您足够幸运地能够使用具有 CUDA 功能的 GPU(您可以从大多数云提供商处以每小时$ 0.50 的价格租用一个 GPU），则可以使用它来加速代码</p>
<p>首先检查您的 GPU 是否在 Pytorch 中正常工作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<p>如果GPU可用，则会有以下输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>然后为其创建一个设备对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dev = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>让我们更新<code>preprocess</code>，将批次移至 GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>最后，我们可以将模型移至 GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>您应该发现它现在运行得更快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">0.2331619877576828</span></span><br><span class="line"><span class="number">1</span> <span class="number">0.2043442727804184</span></span><br></pre></td></tr></table></figure>
<h2 id="总结思想"><a class="header-anchor" href="#总结思想"> </a>总结思想</h2>
<hr>
<p>现在，我们有了一个通用的数据管道和训练循环，您可以将其用于使用 Pytorch 训练多种类型的模型。 要了解现在可以轻松进行模型训练，请查看 mnist_sample 示例笔记本</p>
<p>当然，您需要添加很多内容，例如数据增强，超参数调整，监控训练，转移学习等</p>
<p>这些功能在 fastai 库中可用，该库是使用本教程中所示的相同设计方法开发的，为希望进一步推广模型的从业人员提供了自然的下一步</p>
<p>我们承诺在本教程开始时将通过示例分别说明<code>torch.nn</code>，<code>torch.optim</code>，<code>Dataset</code>和<code>DataLoader</code></p>
<p>因此，让我们总结一下我们所看到的：</p>
<blockquote>
<p><code>torch.nnModule</code>：创建一个类似函数行为功能的，但可以包含状态(例如神经网络层权重）的可调用对象。它知道它包含的<code>Parameter</code>，并且可以将其所有梯度归零，通过其循环进行权重更新等</p>
<p><code>Parameter</code>：张量的包装器，它告诉Module具有在反向传播期间需要更新的权重<br>
仅更新具有 require_grad 属性集的张量</p>
<p><code>functional</code>：一个模块(通常按照常规导入到F名称空间中），包含激活函数，损失函数等<br>
以及卷积和线性层之类的无状态版本</p>
<p><code>torch.optim</code>：包含其中SGD之类的优化程序，这些优化程序可以在反向传播期间更新权重参数</p>
<p><code>Dataset</code>：一个具有<code>__len__</code>和<code>__getitem__</code>的抽象接口对象，包括 Pytorch 提供的类，例如Tensor</p>
<p><code>DatasetDataLoader</code>：获取任何<code>Dataset</code>并创建一个迭代器，该迭代器返回批量数据</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://edwardssss.github.io">砕月之殇</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://edwardssss.github.io/posts/60592/">https://edwardssss.github.io/posts/60592/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://edwardssss.github.io" target="_blank">砕月之殇的摆烂小窝</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="/user/show_myself.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/32820/" title="Pytorch数据并行处理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Pytorch数据并行处理</div></div></a></div><div class="next-post pull-right"><a href="/posts/22555/" title="生活已经够累了，开心一点"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">生活已经够累了，开心一点</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/user/show_myself.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">砕月之殇</div><div class="author-info__description">个人博客站(也是灌水吐槽站)</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Edwardssss"><i class="fab fa-github"></i><span>Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Edwardssss" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ed_129@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://edwardssss.github.io/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><b><font color="#e66b6d">双</font> <font color="#e66d98">手</font> <font color="#e66cc6">合</font> <font color="#cc6de6">十</font> <font color="#9770e6">成</font> <font color="#6d93e6">为</font> <font color="#6fcde6">自</font> <font color="#72e6b6">己</font> <font color="#72e689">的</font> <font color="#99e670">神</font> <br /><font color="#cde670">自</font> <font color="#e6df72">己</font> <font color="#e6c073">所</font> <font color="#e6a271">信</font> <font color="#e6796f">念</font> <font color="#e65454">的</font> <font color="#e63333">即</font> <font color="#e62c2c">是</font> <font color="#e60101">信</font> <font color="#e60101">仰</font></b> <a href="https://smms.app/image/1WNzyQaDAJPSt2I" target="_blank"><img src="https://s2.loli.net/2023/03/12/1WNzyQaDAJPSt2I.png" width=200 /></a></div><div class="xpand" style="height:0px;"></div></div><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas><script src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/twopeople1.js"></script><script src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/zdog.dist.js"></script><script id="rendered-js" src="https://fastly.jsdelivr.net/gh/xiaopengand/blogCdn@latest/xzxr/twopeople.js"></script><style>.card-widget.card-announcement {
margin: 0;
align-items: center;
justify-content: center;
text-align: center;
}
canvas {
display: block;
margin: 0 auto;
cursor: move;
}</style><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#MNIST-%E6%95%B0%E6%8D%AE%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">MNIST 数据设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%97%A0-torch-nn%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">从零开始的神经网络(无 torch.nn）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-torch-nn-functional"><span class="toc-number">3.</span> <span class="toc-text">使用 torch.nn.functional</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-nn-Module-%E8%BF%9B%E8%A1%8C%E9%87%8D%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">使用 nn.Module 进行重构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-nn-Linear-%E9%87%8D%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">使用 nn.Linear 重构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8C%96%E9%87%8D%E6%9E%84"><span class="toc-number">6.</span> <span class="toc-text">使用优化重构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E9%87%8D%E6%9E%84"><span class="toc-number">7.</span> <span class="toc-text">使用数据集进行重构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-DataLoader-%E8%BF%9B%E8%A1%8C%E9%87%8D%E6%9E%84"><span class="toc-number">8.</span> <span class="toc-text">使用 DataLoader 进行重构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E9%AA%8C%E8%AF%81"><span class="toc-number">9.</span> <span class="toc-text">添加验证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-fit-%EF%BC%89%E5%92%8C-get-data-%EF%BC%89"><span class="toc-number">10.</span> <span class="toc-text">创建 fit(）和 get_data(）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E6%8D%A2%E5%88%B0-CNN"><span class="toc-number">11.</span> <span class="toc-text">切换到 CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-Sequential"><span class="toc-number">12.</span> <span class="toc-text">nn.Sequential</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8C%85%E8%A3%85-DataLoader"><span class="toc-number">13.</span> <span class="toc-text">包装 DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%82%A8%E7%9A%84-GPU"><span class="toc-number">14.</span> <span class="toc-text">使用您的 GPU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%80%9D%E6%83%B3"><span class="toc-number">15.</span> <span class="toc-text">总结思想</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/user/bg.jpg')"><div id="footer-wrap"><div class="footer_custom_text">现在是，摆烂时间(开摆)</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://blog-project-2jq5e19sn-edwardssss.vercel.app/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: true,
    }, null))
  }

  if (typeof Waline === 'object') initWaline()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Waline' === 'Waline' || !false) {
  if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><div class="aplayer no-destroy" data-id="8788584251" data-server="tencent" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script type="text/javascript" src="/js/waline.js" ></script><script type="text/javascript" src="/js/giscus.js" ></script><script type="text/javascript" src="/js/fps.js" ></script><script id="canvas_nest" defer="defer" color="255,0,0" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="开摆,睡觉,我好困,饿了,我好累,要睡了" data-fontsize="17px" data-random="true" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false},"rect":"opacity:0.7","log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>